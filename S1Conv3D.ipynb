{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import gc\n",
    "import ast\n",
    "import cv2\n",
    "import time\n",
    "import timm\n",
    "import pickle\n",
    "import random\n",
    "import pydicom\n",
    "import argparse\n",
    "import warnings\n",
    "from glob import glob\n",
    "import nibabel as nib\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import albumentations\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.cuda.amp as amp\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from monai.transforms import Resize\n",
    "import monai.transforms as transforms\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# Set matplotlib to display inline\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 20, 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USE THIS CELL FOR GOOGLE DRIVE COLAB SETUP\n",
    "!pip install -q monai\n",
    "!pip install -q segmentation-models-pytorch==0.2.1\n",
    "!pip install pylibjpeg==1.4.0\n",
    "!pip install python-gdcm==3.0.17.1\n",
    "\n",
    "\n",
    "#\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define the path to the \"data\" folder\n",
    "data_folder = '/content/drive/My Drive/data'\n",
    "\n",
    "# Check if the folder exists\n",
    "if os.path.exists(data_folder):\n",
    "    print(f\"Successfully accessed: {data_folder}\")\n",
    "    print(\"Files in the folder:\")\n",
    "    print(os.listdir(data_folder))  # List files in the folder\n",
    "else:\n",
    "    print(\"The folder 'data' was not found in Google Drive.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug mode\n",
    "DEBUG = True\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Configuration\n",
    "kernel_type = 'timm3d_resnet18d_unet4blocks_128cube_dsv2_flip12_shift3.7_gd1.5_bs4_lr3e-4_20x50ep' # maybe use different in future?\n",
    "load_kernel = None\n",
    "load_last = True\n",
    "n_blocks = 4\n",
    "n_folds = 5\n",
    "backbone = 'resnet18d'\n",
    "\n",
    "# Image sizes\n",
    "image_sizes = [128, 128, 128]\n",
    "resize_transform = Resize(image_sizes)\n",
    "\n",
    "# Training hyperparameters\n",
    "init_lr = 3e-3\n",
    "batch_size = 4\n",
    "drop_rate = 0.0\n",
    "drop_path_rate = 0.0\n",
    "loss_weights = [1, 1]\n",
    "p_mixup = 0.1\n",
    "\n",
    "# Data directories\n",
    "data_dir = data_folder\n",
    "\n",
    "# Other configurations\n",
    "use_amp = True\n",
    "num_workers = 4\n",
    "out_dim = 7\n",
    "n_epochs = 1000\n",
    "\n",
    "# Directories for logs and models\n",
    "log_dir = './logs'\n",
    "model_dir = './models'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "df_train = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
    "\n",
    "# Prepare the mask dataframe\n",
    "mask_files = os.listdir(os.path.join(data_dir, 'segmentations'))\n",
    "df_mask = pd.DataFrame({\n",
    "    'mask_file': mask_files,\n",
    "})\n",
    "df_mask['StudyInstanceUID'] = df_mask['mask_file'].apply(lambda x: x[:-4])\n",
    "df_mask['mask_file'] = df_mask['mask_file'].apply(lambda x: os.path.join(data_dir, 'segmentations', x))\n",
    "\n",
    "# Merge training data with mask data\n",
    "df = df_train.merge(df_mask, on='StudyInstanceUID', how='left')\n",
    "df['image_folder'] = df['StudyInstanceUID'].apply(lambda x: os.path.join(data_dir, 'train_images', x))\n",
    "df['mask_file'].fillna('', inplace=True)\n",
    "\n",
    "# Filter samples with masks\n",
    "df_seg = df.query('mask_file != \"\"').reset_index(drop=True)\n",
    "\n",
    "# Define cv folds\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "df_seg['fold'] = -1\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(df_seg)):\n",
    "    df_seg.loc[valid_idx, 'fold'] = fold\n",
    "\n",
    "# Display the last few entries to see if it makes sense\n",
    "df_seg.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of StudyInstanceUIDs to revert masks\n",
    "revert_list = [\n",
    "    '1.2.826.0.1.3680043.1363',\n",
    "    '1.2.826.0.1.3680043.20120',\n",
    "    '1.2.826.0.1.3680043.2243',\n",
    "    '1.2.826.0.1.3680043.24606',\n",
    "    '1.2.826.0.1.3680043.32071'\n",
    "]\n",
    "\n",
    "def load_dicom(path):\n",
    "    dicom = pydicom.read_file(path)\n",
    "    data = dicom.pixel_array\n",
    "    data = cv2.resize(data, (image_sizes[0], image_sizes[1]), interpolation=cv2.INTER_LINEAR)\n",
    "    return data\n",
    "\n",
    "def load_dicom_line_par(path):\n",
    "    t_paths = sorted(glob(os.path.join(path, \"*\")),\n",
    "                     key=lambda x: int(os.path.basename(x).split(\".\")[0]))\n",
    "    \n",
    "    n_scans = len(t_paths)\n",
    "    indices = np.quantile(list(range(n_scans)), np.linspace(0., 1., image_sizes[2])).round().astype(int)\n",
    "    t_paths = [t_paths[i] for i in indices]\n",
    "    \n",
    "    images = []\n",
    "    for filename in t_paths:\n",
    "        images.append(load_dicom(filename))\n",
    "    images = np.stack(images, -1)\n",
    "    \n",
    "    # Normalize images\n",
    "    images = images - np.min(images)\n",
    "    images = images / (np.max(images) + 1e-4)\n",
    "    images = (images * 255).astype(np.uint8)\n",
    "\n",
    "    return images\n",
    "\n",
    "def load_sample(row, has_mask=True):\n",
    "    image = load_dicom_line_par(row.image_folder)\n",
    "    if image.ndim < 4:\n",
    "        image = np.expand_dims(image, 0).repeat(3, 0)  # to 3 channels\n",
    "    \n",
    "    if has_mask:\n",
    "        mask_org = nib.load(row.mask_file).get_fdata()\n",
    "        shape = mask_org.shape\n",
    "        mask_org = mask_org.transpose(1, 0, 2)[::-1, :, ::-1]  # (d, w, h)\n",
    "        mask = np.zeros((7, shape[0], shape[1], shape[2]))\n",
    "        for cid in range(7):\n",
    "            mask[cid] = (mask_org == (cid+1))\n",
    "        mask = (mask * 255).astype(np.uint8)\n",
    "        mask = resize_transform(mask).numpy()\n",
    "        \n",
    "        return image, mask\n",
    "    else:\n",
    "        return image\n",
    "\n",
    "class SEGDataset(Dataset):\n",
    "    def __init__(self, df, mode, transform):\n",
    "        self.df = df.reset_index()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        \n",
    "        image, mask = load_sample(row, has_mask=True)\n",
    "    \n",
    "        if row.StudyInstanceUID in revert_list:\n",
    "            mask = mask[:, :, :, ::-1]\n",
    "\n",
    "        if self.transform:\n",
    "            res = self.transform({'image': image, 'mask': mask})\n",
    "            image = res['image'] / 255.\n",
    "            mask = res['mask']\n",
    "            mask = (mask > 127).astype(np.float32)\n",
    "    \n",
    "        image, mask = torch.tensor(image).float(), torch.tensor(mask).float()\n",
    "    \n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentations for training\n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.RandFlipd(keys=[\"image\", \"mask\"], prob=0.5, spatial_axis=1),\n",
    "    transforms.RandFlipd(keys=[\"image\", \"mask\"], prob=0.5, spatial_axis=2),\n",
    "    transforms.RandAffined(\n",
    "        keys=[\"image\", \"mask\"],\n",
    "        translate_range=[int(x * y) for x, y in zip(image_sizes, [0.3, 0.3, 0.3])],\n",
    "        padding_mode='zeros',\n",
    "        prob=0.7\n",
    "    ),\n",
    "    transforms.RandGridDistortiond(\n",
    "        keys=(\"image\", \"mask\"),\n",
    "        prob=0.5,\n",
    "        distort_limit=(-0.01, 0.01),\n",
    "        mode=\"nearest\"\n",
    "    ),    \n",
    "])\n",
    "\n",
    "# No augmentations for validation\n",
    "transforms_valid = transforms.Compose([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset for visualization\n",
    "df_show = df_seg\n",
    "dataset_show = SEGDataset(df_show, 'train', transform=transforms_train)\n",
    "\n",
    "# Plot samples\n",
    "rcParams['figure.figsize'] = 20, 8\n",
    "for i in range(2):\n",
    "    fig, axarr = plt.subplots(1, 4)\n",
    "    for p in range(4):\n",
    "        idx = i * 4 + p\n",
    "        img, mask = dataset_show[idx]\n",
    "        img_slice = img[:, :, :, 60]\n",
    "        mask_slice = mask[:, :, :, 60]\n",
    "        \n",
    "        # Combine masks\n",
    "        mask_slice[0] = mask_slice[0] + mask_slice[3] + mask_slice[6]\n",
    "        mask_slice[1] = mask_slice[1] + mask_slice[4]\n",
    "        mask_slice[2] = mask_slice[2] + mask_slice[5]\n",
    "        mask_slice = mask_slice[:3]\n",
    "        \n",
    "        # Overlay mask on image\n",
    "        img_overlay = img_slice * 0.7 + mask_slice * 0.3\n",
    "        axarr[p].imshow(img_overlay.transpose(0, 1).transpose(1, 2).squeeze(), cmap='gray')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define the segmentation model using Timm\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTimmSegModel\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, backbone, segtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munet\u001b[39m\u001b[38;5;124m'\u001b[39m, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m(TimmSegModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the segmentation model using Timm\n",
    "class TimmSegModel(nn.Module):\n",
    "    def __init__(self, backbone, segtype='unet', pretrained=False):\n",
    "        super(TimmSegModel, self).__init__()\n",
    "\n",
    "        self.encoder = timm.create_model(\n",
    "            backbone,\n",
    "            in_chans=3,\n",
    "            features_only=True,\n",
    "            drop_rate=drop_rate,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "        g = self.encoder(torch.rand(1, 3, 64, 64))\n",
    "        encoder_channels = [1] + [_.shape[1] for _ in g]\n",
    "        decoder_channels = [256, 128, 64, 32, 16]\n",
    "        if segtype == 'unet':\n",
    "            self.decoder = smp.unet.decoder.UnetDecoder(\n",
    "                encoder_channels=encoder_channels[:n_blocks+1],\n",
    "                decoder_channels=decoder_channels[:n_blocks],\n",
    "                n_blocks=n_blocks,\n",
    "            )\n",
    "\n",
    "        self.segmentation_head = nn.Conv3d(\n",
    "            decoder_channels[n_blocks-1],\n",
    "            out_dim,\n",
    "            kernel_size=(3, 3, 3),\n",
    "            stride=(1, 1, 1),\n",
    "            padding=(1, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        global_features = [0] + self.encoder(x)[:n_blocks]\n",
    "        seg_features = self.decoder(*global_features)\n",
    "        seg_features = self.segmentation_head(seg_features)\n",
    "        return seg_features\n",
    "\n",
    "from timm.models.layers.conv2d_same import Conv2dSame\n",
    "from conv3d_same import Conv3dSame  # Ensure this module is available or defined\n",
    "\n",
    "def convert_3d(module):\n",
    "    \"\"\"Recursively convert 2D modules to 3D.\"\"\"\n",
    "    module_output = module\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "        module_output = torch.nn.BatchNorm3d(\n",
    "            module.num_features,\n",
    "            module.eps,\n",
    "            module.momentum,\n",
    "            module.affine,\n",
    "            module.track_running_stats,\n",
    "        )\n",
    "        if module.affine:\n",
    "            with torch.no_grad():\n",
    "                module_output.weight = module.weight\n",
    "                module_output.bias = module.bias\n",
    "        module_output.running_mean = module.running_mean\n",
    "        module_output.running_var = module.running_var\n",
    "        module_output.num_batches_tracked = module.num_batches_tracked\n",
    "        if hasattr(module, \"qconfig\"):\n",
    "            module_output.qconfig = module.qconfig\n",
    "\n",
    "    elif isinstance(module, Conv2dSame):\n",
    "        module_output = Conv3dSame(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.Conv2d):\n",
    "        module_output = torch.nn.Conv3d(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "            padding_mode=module.padding_mode\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.MaxPool2d):\n",
    "        module_output = torch.nn.MaxPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            dilation=module.dilation,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "    elif isinstance(module, torch.nn.AvgPool2d):\n",
    "        module_output = torch.nn.AvgPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "\n",
    "    for name, child in module.named_children():\n",
    "        module_output.add_module(\n",
    "            name, convert_3d(child)\n",
    "        )\n",
    "    del module\n",
    "\n",
    "    return module_output\n",
    "\n",
    "# Initialize and convert the model\n",
    "model = TimmSegModel(backbone, pretrained=True)\n",
    "model = convert_3d(model)\n",
    "model.to(device)\n",
    "\n",
    "# Test the model with a random input\n",
    "test_input = torch.rand(1, 3, *image_sizes).to(device)\n",
    "test_output = model(test_input)\n",
    "print(test_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOSS FUNCTION AND METRICS\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "def binary_dice_score(\n",
    "    y_pred: torch.Tensor,\n",
    "    y_true: torch.Tensor,\n",
    "    threshold: Optional[float] = None,\n",
    "    nan_score_on_empty=False,\n",
    "    eps: float = 1e-7,\n",
    ") -> float:\n",
    "    if threshold is not None:\n",
    "        y_pred = (y_pred > threshold).to(y_true.dtype)\n",
    "\n",
    "    intersection = torch.sum(y_pred * y_true).item()\n",
    "    cardinality = (torch.sum(y_pred) + torch.sum(y_true)).item()\n",
    "\n",
    "    score = (2.0 * intersection) / (cardinality + eps)\n",
    "\n",
    "    has_targets = torch.sum(y_true) > 0\n",
    "    has_predicted = torch.sum(y_pred) > 0\n",
    "\n",
    "    if not has_targets:\n",
    "        if nan_score_on_empty:\n",
    "            score = np.nan\n",
    "        else:\n",
    "            score = float(not has_predicted)\n",
    "    return score\n",
    "\n",
    "def multilabel_dice_score(\n",
    "    y_true: torch.Tensor,\n",
    "    y_pred: torch.Tensor,\n",
    "    threshold=None,\n",
    "    eps=1e-7,\n",
    "    nan_score_on_empty=False,\n",
    "):\n",
    "    ious = []\n",
    "    num_classes = y_pred.size(0)\n",
    "    for class_index in range(num_classes):\n",
    "        iou = binary_dice_score(\n",
    "            y_pred=y_pred[class_index],\n",
    "            y_true=y_true[class_index],\n",
    "            threshold=threshold,\n",
    "            nan_score_on_empty=nan_score_on_empty,\n",
    "            eps=eps,\n",
    "        )\n",
    "        ious.append(iou)\n",
    "\n",
    "    return ious\n",
    "\n",
    "def dice_loss(input, target):\n",
    "    input = torch.sigmoid(input)\n",
    "    smooth = 1.0\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    return 1 - ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n",
    "\n",
    "def bce_dice(input, target, loss_weights=loss_weights):\n",
    "    loss1 = loss_weights[0] * nn.BCEWithLogitsLoss()(input, target)\n",
    "    loss2 = loss_weights[1] * dice_loss(input, target)\n",
    "    return (loss1 + loss2) / sum(loss_weights)\n",
    "\n",
    "# Set the criterion\n",
    "criterion = bce_dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING and VALIDATION FUNCTIONS\n",
    "def mixup(input, truth, clip=[0, 1]):\n",
    "    indices = torch.randperm(input.size(0))\n",
    "    shuffled_input = input[indices]\n",
    "    shuffled_labels = truth[indices]\n",
    "\n",
    "    lam = np.random.uniform(clip[0], clip[1])\n",
    "    input = input * lam + shuffled_input * (1 - lam)\n",
    "    return input, truth, shuffled_labels, lam\n",
    "\n",
    "def train_func(model, loader_train, optimizer, scaler=None):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    bar = tqdm(loader_train, desc='Training')\n",
    "    for images, gt_masks in bar:\n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device)\n",
    "        gt_masks = gt_masks.to(device)\n",
    "\n",
    "        do_mixup = False\n",
    "        if random.random() < p_mixup:\n",
    "            do_mixup = True\n",
    "            images, gt_masks, gt_masks_sfl, lam = mixup(images, gt_masks)\n",
    "\n",
    "        with amp.autocast(enabled=use_amp):\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, gt_masks)\n",
    "            if do_mixup:\n",
    "                loss2 = criterion(logits, gt_masks_sfl)\n",
    "                loss = loss * lam + loss2 * (1 - lam)\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        bar.set_description(f'Train Loss: {np.mean(train_loss[-30:]):.4f}')\n",
    "\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "def valid_func(model, loader_valid):\n",
    "    model.eval()\n",
    "    valid_loss = []\n",
    "    ths = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "    batch_metrics = [[] for _ in ths]\n",
    "    bar = tqdm(loader_valid, desc='Validation')\n",
    "    with torch.no_grad():\n",
    "        for images, gt_masks in bar:\n",
    "            images = images.to(device)\n",
    "            gt_masks = gt_masks.to(device)\n",
    "\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, gt_masks)\n",
    "            valid_loss.append(loss.item())\n",
    "\n",
    "            for thi, th in enumerate(ths):\n",
    "                pred = (torch.sigmoid(logits) > th).float()\n",
    "                for i in range(logits.shape[0]):\n",
    "                    tmp = multilabel_dice_score(\n",
    "                        y_pred=pred[i].cpu(),\n",
    "                        y_true=gt_masks[i].cpu(),\n",
    "                        threshold=0.5,\n",
    "                    )\n",
    "                    batch_metrics[thi].extend(tmp)\n",
    "            bar.set_description(f'Valid Loss: {np.mean(valid_loss[-30:]):.4f}')\n",
    "\n",
    "    metrics = [np.mean(this_metric) for this_metric in batch_metrics]\n",
    "    best_th = ths[np.argmax(metrics)]\n",
    "    best_dc = np.max(metrics)\n",
    "    print(f'Best Threshold: {best_th}, Best Dice Coefficient: {best_dc:.4f}')\n",
    "\n",
    "    return np.mean(valid_loss), best_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer and scheduler for visualization\n",
    "optimizer = optim.AdamW(model.parameters(), lr=init_lr)\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)\n",
    "lrs = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    scheduler_cosine.step()\n",
    "    lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "# Plot the learning rate schedule\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(range(len(lrs)), lrs)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Cosine LR scheduler')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fold(fold):\n",
    "    log_file = os.path.join(log_dir, f'{kernel_type}.txt')\n",
    "    model_file = os.path.join(model_dir, f'{kernel_type}_fold{fold}_best.pth')\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    train_df = df_seg[df_seg['fold'] != fold].reset_index(drop=True)\n",
    "    valid_df = df_seg[df_seg['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "    # Initialize datasets and dataloaders\n",
    "    dataset_train = SEGDataset(train_df, 'train', transform=transforms_train)\n",
    "    dataset_valid = SEGDataset(valid_df, 'valid', transform=transforms_valid)\n",
    "    loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    loader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    # Model\n",
    "    model = TimmSegModel(backbone, pretrained=True)\n",
    "    model = convert_3d(model)\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialize optimizer and scaler\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=init_lr)\n",
    "    scaler = amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "    # Initialize scheduler\n",
    "    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=n_epochs)\n",
    "\n",
    "    # Training state variables\n",
    "    metric_best = 0.0\n",
    "    loss_min = np.inf\n",
    "\n",
    "    print(f'Starting Fold {fold}')\n",
    "    print(f'Training samples: {len(dataset_train)}, Validation samples: {len(dataset_valid)}')\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        scheduler_cosine.step(epoch - 1)\n",
    "\n",
    "        print(f'\\nEpoch {epoch}/{n_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Training\n",
    "        train_loss = train_func(model, loader_train, optimizer, scaler)\n",
    "\n",
    "        # Validation \n",
    "        valid_loss, metric = valid_func(model, loader_valid)\n",
    "\n",
    "        # Logging\n",
    "        content = f'{time.ctime()} Fold {fold}, Epoch {epoch}, LR: {optimizer.param_groups[0][\"lr\"]:.7f}, Train Loss: {train_loss:.5f}, Valid Loss: {valid_loss:.5f}, Metric: {metric:.6f}.'\n",
    "        print(content)\n",
    "        with open(log_file, 'a') as appender:\n",
    "            appender.write(content + '\\n')\n",
    "\n",
    "        # Save best model\n",
    "        if metric > metric_best:\n",
    "            print(f'Improvement from {metric_best:.6f} to {metric:.6f}. Saving model...')\n",
    "            torch.save(model.state_dict(), model_file)\n",
    "            metric_best = metric\n",
    "\n",
    "        # Save the last model\n",
    "        if not DEBUG:\n",
    "            torch.save(\n",
    "                {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scaler_state_dict': scaler.state_dict() if scaler else None,\n",
    "                    'score_best': metric_best,\n",
    "                },\n",
    "                model_file.replace('_best', '_last')\n",
    "            )\n",
    "\n",
    "    # Cleanup\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(n_folds):\n",
    "    run_fold(fold)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seg3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
